%% conclusion.tex
%%

%% ==================
\chapter{Conclusion}
\label{ch:Conclusion}
%% ==================

In this section, the conclusions from the evaluation section are drawn and potential limitations of the result are discussed. Furthermore, lessons learned from the experiment are described and potential fields of further research are examined.

\paragraph{\acf{AMT} and the quality of data}
As indicated by the findings of the \acl{NP}, the data generated on \ac{AMT} includes a lot of noise. Many participants failed to get a result higher than 80\%, a level which can be considered as fairly easy to achieve. Even increasing the leverage of the bonus and the total possible payout did not show to have an influence on the participants. So we must conclude, that offering the experiment as a \ac{HIT} on \ac{AMT} will return a lot of data that is not usable for a statistical analysis. 
%The advantages of a computer-lab experiment therefore lay in the quality of the data since the intrinsic motivation of participants is likely to be higher.

\paragraph{The \textit{FirstResult} is the best on a medium level of information granularity, \textit{DecisionTime} shows a maximum and \textit{DecisionNumber} a minimum.}
When focusing the statistical analysis on the participants who made an effort to succeed in the game, the results indicate that an information overload has materialized. In fact, the highest performance is achieved on a medium level of information granularity for the first result that the participants offer. These findings are in particular interesting because we argue that users of smart meters have an interest in achieving a satisfying result in a minimum time, since they divide their time between many things.\\
The findings for the \textit{DecisionTime} indicate that users facing a medium level of information granularity take the most time to make a decision. These results show a potential information overload and can be explained by Jacoby's thesis that one must differentiate between the questions "Can consumers be overloaded?" and "Will consumers be overloaded?".\\ 
Individuals facing an information overload might not directly experience this information overload while making their choice, because they only concentrate on a part of the given information prior to their decision. In other words, individuals confronted with an information overload are selective in their information selection and stop \textit{"far short of overloading themselves"} \citep{Jacoby1984}.\\
A participant on a medium level of information granularity is just at the border of an information overload - the individual can still cope with the amount of information. Since the given information provides more detail than a lower level of information granularity, we can define three consequences:
\begin{itemize}
\item The choice is supported by a better understanding of the underlying parameter (Benefit) and therefore the choice accuracy increases $\Rightarrow$ \textit{FirstResult}.
\item Due to the amount of information to process, it takes the individual more time to make the decision $\Rightarrow$ \textit{DecisionTime}.
\item Since the choice accuracy increases, the individual makes fewer but better decisions $\Rightarrow$ \textit{DecisionNumber}.
\end{itemize}
When being provided with more information, the information processing gets selective, so the actual amount of information that is considered for the choice might decrease. Therefore, both the choice accuracy and the decision time decrease, and individuals might not base their decisions on a profound assessment of the options, but rather play around with different choices. Consequently, the number of decisions increases.\\
In addition, when excluding the lower levels of information granularity, the results of the statistical model indicate that the best performance can be achieved on a medium-to-high level of information granularity. These findings are not described in our original hypothesis, but have implications for the further development of the experiment.\\
A medium level of information might be lower when the aim is to reach a satisfying result as soon as possible, but gets higher when one aims for the best solution. Therefore, a future experiment might include setups with more colours, in order to examine if the best performances further decrease with an increasing number of colours. 

\paragraph{Learning Effect has a influence on the performance and the game time} 

Participants improve their performance with a growing number of played rounds, yet the magnitude of the learning effect diminishes over the number of rounds. So for the identified information overload in reaching the \textit{FirstResult}, we can conclude that individuals find strategies to cope with information overload when they get more experienced in this situation. A potential question to ask for future is how these findings are reflected in a setup that includes more rounds.

\paragraph{Individual characteristics affect the performance}
Individual characteristics has proven to be influential on the participants. Even tough participants might show a similar sensitivity to the level of information granularity, their performances are affected by individual factors. Taking into account these effects when setting up a statistical model helps to exclude influences such as limited attention while playing the game or an individual talent for succeeding in these types of experiments. As a result, limitations of the \acl{AMT} and there negative consequences on statistical results can be tackled by the design of the statistical model.

\paragraph{A higher leverage on the bonus does not improve performances.}
The adjusted bonus system in Trial 2 has not proven to have an influence on the performances of the participants. Reasons might be related to an unclear explanation so individuals were not aware of the monetary incentives when reaching the bonus bar of 80\%.
%
%One reason for the lack of a detectable learning effect might be the trial round prior to the game. This opportunity enabled the participants to spend as much time as desired on exploring the experiment and on trying different strategies. As indicated by the timelog analysis in Section \ref{ch:Experiment:sec:DataacquisitionDescriptives}, the participants spent the majority of their time on the explanation part and used the chance to get familiar with the experiment. The trial round was necessary to ensure that participants were fully aware of the task and the design of the experiment. Yet, the learning curve already started here, participants could improve their performance without any time pressure and were used to the implemented information overload. The findings for the final performance from Round 1 and 2 might be an indication that the learning effect continued for the first 2 rounds. Having said that, one would expect the final performance for Round 3 to be at least the level of the performance in Round 2, yet usergroup 3 and 4 reduce their performance in Round 3.\\
%The lack of a learning effect is interesting since prior research with a similar experiment \citep{Schmidt2012} gave an indication that a learning effect is significant. In contrast to our setup, Schmidt's experiment included 13 runs, so 
%
%excluding those participants who were not able to perform better than 80\%
%
%
%As indicated by the findings of the \acl{NP}, the data generated on \ac{AMT} includes a lot of noise. Many participants failed to get a result higher than 80\%, a level which can be considered as fairly easy to achieve. Even increasing the leverage of the bonus and the total possible payout did not show to have an influence on the participants. So we must conclude, that offering the experiment as a \ac{HIT} on \ac{AMT} will return a lot of data that is not usable for the analysis of statistical implications. The advantages of a computer-lab experiment clearly lay in the quality of the data.
%
%
%
%
%\section{Information Granularity}
%
%The influence of the information granularity has not shown the expected magnitude since all the related hypotheses are rejected. Nevertheless, a comparison between a low information granularity and a high information granularity - usergroup 1 and 4 - returns significant results, indicating, that a low information granularity leads to better choice accuracy. 
%
%In order to back up these findings, further research might add more colours to the framework to prove the negative correlation between number of colours and performance. A small change for the number of colours between treatments did not emerge to be the right approach to get distinctive results for each treatment, so we would suggest to increase the number of colours with a larger step size, e.g. introducing 27 or more colours. This approach might be more successful in finding a significant relationship between information granularity and choice accuracy.\\
%Yet, the reason for the lack of significant results for a medium information granularity must be further discussed. When the number of colours is increased, two opposing trends in the complexity of the game can be identified. As the number of colours increases, the color frame gets more complex and the dimension of the attribute level \textit{benefit} increases. This development reinforces the information load, as indicated by \cite{Jacoby1974} and \cite{Malhotra1982}. In contrast to that, with a growing number of colours, the benefit range covered by one colour decreases, so the choices get more specific and accurate. Whereas participant 1 who faces a low number of colours can choose between colours that represent a wide range of benefit, participant 2 who is challenged with a high number of colours can choose among colours that represent a more specific benefit value. Consequently, the participant 2 is less dependent on the chance that he or she clicks a box with a high benefit within the benefit range, but can make a more conscious choice.\\
%These two opposing trends might be one reason why there is no clear development detectable for increasing number of colours. Reducing the complexity of the game to a controllable, one-dimensional approach would be a potential for future research. In the forefront of the experiment, several interfaces were open for discussion. The implemented approach was chosen because it guaranteed the comparability among usergroups, since the underlying knapsack problems are the same. If future research finds a way to combine comparability and one-dimensional complexity, the findings might get clearer for the influence from information granularity. One approach would be to assign one specific value per colour. This will result in different benefit ranges for each treatment and challenge the comparability of the problems. Yet, by using the delta-benchmark that was applied for the comparability between rounds, researchers can control the difficulty of the underlying problems and can consequently control comparability.
%
%\section{Learning Effect}
%
%

\paragraph{Limitations of the experiment}
%As stated in Section \ref{ch:Evaluation:sec:DescriptiveStatistics:subsec:Distributionofusergroups}, the distribution of participants among usergroup was not uniformly. Since we saw a higher dropout-rate for usergroups with a higher number of colour, the representation of the population mean in each usergroup might be diluted. As participants drop out of usergroups 2, 3 and 4, the data is skewed towards participant in usergroup who face a lower information load. Our findings would be therefore suspect, if the performance of usergroup 1 was worse than the performance of all usergroups since the better performance of usergroup 2, 3 and 4 might be related to the dropout of participants who were unable to cope with the information overload.
In order to make the experiment as intuitive and straight-forward as possible, a considerable level of abstraction is necessary. This helps participants to better understand the given task, but also implies a simplification of the information environment the real-life smart-meter user is usually confronted with\footnote{Refer to \cite{Jacoby1984}}. The experiment has proven an information overload in the simplified game environment. So we can argue that these results lead to the conclusion that an information overload would occur under a more complex experiment setup, e.g. analysing the data provided by a smart-meter. 
In contrast to that, the undetectable information overload in the simplified experiment for the best performances of a participants does not lead to the conclusion that there will not be an information overload in a more complex experiment environment.


%
%
%The design of the experiment requires a
%Oversimplification of information overload experiments 433 \cite{Jacoby1984}
%
%Control group with 3, 7, 11, 15
%
%No Random treatment assignment!! ==> No representation of population mean
%\dots

%
%
%\begin{table}[p]%
%\centering
%\parbox{0.4\textwidth}{
%\begin{footnotesize}
%\begin{tabular}{l D{.}{.}{3.5} @{}}
%\toprule
%                 & \multicolumn{1}{c}{Parameters} \\
%\midrule
%(Intercept)      & -4.08^{\cdot} \\
%                 & (1.90)        \\
%NumberOfColours  & 1.84^{**}     \\
%                 & (0.59)        \\
%NumberOfColours2 & -0.10^{*}     \\
%                 & (0.03)        \\
%\midrule
%R$^2$            & 0.49          \\
%Adj. R$^2$       & 0.41          \\
%Num. obs.        & 15            \\
%\bottomrule
%\vspace{-3mm}\\
%\multicolumn{2}{l}{\textsuperscript{***}$p<0.001$, 
%  \textsuperscript{**}$p<0.01$, 
%  \textsuperscript{*}$p<0.05$, 
%  \textsuperscript{$\cdot$}$p<0.1$}
%\end{tabular}
%\end{footnotesize}
%\caption{Parameter estimations}
%\label{tab:table}
%}
%\qquad
%\begin{minipage}[c]{0.4\textwidth}%
%\centering
%    \includegraphics[width=1\textwidth]{PlotColoursLearningEffect.pdf}
%\caption{Plot Parameters over NumberOfColours}
%\label{fig:figure}
%\end{minipage}
%\end{table}